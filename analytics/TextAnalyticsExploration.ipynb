{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "report_url = \"http://hansardpublic.parliament.sa.gov.au/Pages/HansardResult.aspx#/docid/HANSARD-10-25756\"\n",
    "report_text = \"When I tried to get a copy of the commissioner's report after being tabled, why was I basically told that there was a very limitedâ€” The PRESIDENT: This is a matter of personal explanation in a supplementary. Just please, the Hon. Mr Wortley, ask your supplementary. The Hon. R.P. WORTLEY: Why weren't all members of parliament given a copy of the royal commission's report? The Hon. D.W. Ridgway: But you told us before you never read reports. The Hon. R.I. LUCAS (Treasurer) (15:26): Mr President, I won't go down that particular path, as delicious as that interjection might have been in relation to the Hon. Mr Wortley saying he couldn't trust himself to read his own reports. I don't know why the Hon. Mr Wortley was unable to get a copy of the royal commission report. It was certainly publicly available. If it pleases the member, I will see whether there is not a spare copy somewhere. If we do find a spare copy and give it to him, I will be asking questions afterwards of the honourable member just to make sure he did read it. The Hon. D.W. Ridgway: Do you want it delivered to Scuzzi or something more convenient for you? The PRESIDENT: Are you finished, the Hon. Mr Ridgway? The Hon. R.P. WORTLEY: You just worry about our trade exports, mate, for the state. The PRESIDENT: The Hon. Mr Wortley, I am waiting patiently here to give you the call for your question. Have you finished your private conversation with the Hon. Mr Ridgway? Yes? The Hon. Mr Wortley.\"\n",
    "report_title = \"Murray-Darling Basin Royal Commission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Import data from spreadsheet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel (\"..\\\\data\\\\HANSARDfullDataset.xlsx\", sheet_name=\"text\")\n",
    "df = pd.DataFrame(data, columns= ['hansardID','text'])\n",
    "df = df.astype({\"hansardID\":'str', \"text\":'str'}) \n",
    "\n",
    "#df.dtypes \n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#hansardFilesInfo = pd.read_excel (\"..\\\\data\\\\HANSARDfullDataset.xlsx\", sheet_name=\"HANSARDFilesInfo\")\n",
    "#hansardFilesInfo = pd.DataFrame(hansardFilesInfo, columns= ['FileName','URL'])\n",
    "#hansardFilesInfo = hansardFilesInfo.astype({\"FileName\":'str', \"URL\":'str'}) \n",
    "#hansardFilesInfo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#header = pd.read_excel (\"..\\\\data\\\\HANSARDfullDataset.xlsx\", sheet_name=\"header\")\n",
    "#header = pd.DataFrame(header)\n",
    "#header.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#bill = pd.read_excel (\"..\\\\data\\\\HANSARDfullDataset.xlsx\", sheet_name=\"bill\")\n",
    "#bill = pd.DataFrame(bill, columns= ['question','bname'])\n",
    "#bill.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Group text into one document\n",
    "grouped_text = df.groupby('hansardID')['text'].agg(lambda col: '. '.join(col))\n",
    "grouped_text_df = pd.DataFrame(grouped_text, columns= ['text'])\n",
    "grouped_text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "grouped_text.iloc[4].replace(\"..\",\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Count number of words in sentence using regex\n",
    "grouped_text_df['WordCount'] = grouped_text_df.apply(lambda x: len(re.findall(r'\\w+', x.text)), axis=1)\n",
    "grouped_text_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "\n",
    "all_text = df['text'].agg(lambda col: ''.join(col))\n",
    "all_text = \". \".join(all_text)\n",
    "\n",
    "# Average number of words in a sentence\n",
    "parts = [len(l.split()) for l in re.split(r'[?!.:] ', ' '.join(all_text)) if l.strip()]\n",
    "print(\"Average = \", sum(parts)/len(parts))\n",
    "\n",
    "# Median number of words in a sentence\n",
    "print(\"Median = \", median(parts))\n",
    "print(\"Min = \", min(parts))\n",
    "print(\"Max = \", max(parts))\n",
    "print(\"Q1 = \", np.percentile(parts,25))\n",
    "print(\"Q3 = \", np.percentile(parts,75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles and libraries to look into further: \n",
    "* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "* https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "* https://github.com/alanbuxton/PyTeaserPython3\n",
    "* https://github.com/abisee/pointer-generator\n",
    "* https://github.com/DerwenAI/pytextrank\n",
    "* https://github.com/tensorflow/models/tree/master/research/textsum\n",
    "* https://radimrehurek.com/gensim/models/lsimodel.html\n",
    "* https://towardsdatascience.com/text-summarization-in-python-76c0a41f0dc4 (additional links to articles at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Base Text Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# The feature base model extracts the features of the sentence, then evaluate its importance\n",
    "# Feature base text summarization by TextTeaser\n",
    "#from pyteaser import SummarizeUrl\n",
    "#url = 'http://www.huffingtonpost.com/2013/11/22/twitter-forward-secrecy_n_4326599.html'\n",
    "#summaries = SummarizeUrl(url)\n",
    "#print summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TextTeasor - automatic summarization algorithm that combines the power of natural language processing and machine learning\n",
    "#from textteaser import TextTeaser\n",
    "#tt = TextTeaser()\n",
    "#tt.summarize(title, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Topic Model summarisation\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "model = LsiModel(common_corpus, id2word=common_dictionary)\n",
    "vectorized_corpus = model[common_corpus]\n",
    "#print(vectorized_corpus)\n",
    "#model.print_topics(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def smart_truncate(content, length=200, suffix='...'):\n",
    "    if len(content) <= length:\n",
    "        return content\n",
    "    else:\n",
    "        return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix\n",
    "\n",
    "smart_truncate(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_text_df['Truncated'] = grouped_text_df.apply(lambda x: smart_truncate(x.text), axis=1)\n",
    "grouped_text_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# https://rare-technologies.com/text-summarization-with-gensim/\n",
    "#\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def get_keywords(text):\n",
    "    keyword_list = keywords(text, split=True, lemmatize=True, deacc=True)\n",
    "    return ', '.join(keyword_list[:10])\n",
    "\n",
    "grouped_text_df['KeyWords'] = grouped_text_df.apply(lambda x: \n",
    "                                                    get_keywords(x.text), \n",
    "                                                    axis=1)\n",
    "grouped_text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "get_keywords(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/DerwenAI/pytextrank\n",
    "# https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb\n",
    "# Requires JSON input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "#  TextRank summarization with default parameters\n",
    "grouped_text_df['TextRank'] = grouped_text_df.apply(lambda x: \n",
    "                                                    summarize(x.text).replace(\"\\n\",\" \").replace(\"..\",\".\"), \n",
    "                                                    axis=1)\n",
    "\n",
    "#  TextRank summarization with no more than 50 words for the summary\n",
    "grouped_text_df['TextRank50'] = grouped_text_df.apply(lambda x: \n",
    "                                                      summarize(x.text, word_count = 50).replace(\"\\n\",\" \").replace(\"..\",\".\"), \n",
    "                                                      axis=1)\n",
    "grouped_text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(summarize(report_text)) #  TextRank summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(summarize(report_text, word_count = 50)) #  TextRank summarization - no more than 50 words for summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(summarize(report_text, ratio = 0.2)) #  TextRank summarization - use no more than 20% of original text for summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "summarize(grouped_text.iloc[4], word_count = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "summarize(grouped_text.iloc[2], word_count = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_rank_summary(text):\n",
    "    \n",
    "    if len(text) <= 200:\n",
    "        return text\n",
    "    \n",
    "    sentences = []    \n",
    "    text_sentences = re.split(r'[?!.] ', text)\n",
    "    \n",
    "    for sentence in text_sentences:\n",
    "        processed = sentence.replace(\"[^a-zA-Z]\", \" \")\n",
    "        word_count = len(re.findall(r'\\w+', processed)) \n",
    "        if word_count > 1: # Include sentences with more than one word\n",
    "            sentences.append(processed)\n",
    "    \n",
    "    summary = summarize('. '.join(sentences))             \n",
    "    return summary.replace(\"\\n\",\" \").replace(\"..\",\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#  TextRank summarisation with no more than 50 words for the summary\n",
    "grouped_text_df['TextRankProcessed'] = grouped_text_df.apply(lambda x: generate_text_rank_summary(x.text), axis=1)\n",
    "grouped_text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generate_text_rank_summary(grouped_text.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generate_text_rank_summary(grouped_text.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generate_text_rank_summary(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "# Approach uses TextRank algorithm\n",
    "# TextRank does not rely on any previous training data and can work with any arbitrary piece of text. \n",
    "# TextRank is a general purpose graph-based ranking algorithm for NLP\n",
    "\n",
    "# Import all necessary libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer # is based on The Porter Stemming Algorithm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re \n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Generate clean sentences\n",
    "def create_long_sentences(article, output=True):\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in article:\n",
    "        word_count = len(re.findall(r'\\w+', sentence)) \n",
    "        if word_count > 4: # Include sentences with more than four words\n",
    "            sentences.append(sentence.split(\" \"))\n",
    "        if output: \n",
    "            print(sentence, \": words = \", word_count)\n",
    "        \n",
    "    return sentences   \n",
    "\n",
    "\n",
    "# Process word by converting to lowercase, removing punctuation, lemmatising, and stemming. \n",
    "# https://medium.com/@pemagrg/pre-processing-text-in-python-ad13ea544dae\n",
    "def process_word(word):\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    word = word.lower() # convert to lowercase\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation)) # remove punctuation \n",
    "    word = wordnet_lemmatizer.lemmatize(word) # lemmatise words\n",
    "    word = snowball_stemmer.stem(word) # stemming\n",
    "    \n",
    "    return word\n",
    "\n",
    "\n",
    "# Process sentence by converting to lowercase, removing punctuation, lemmatising, stemming and removing stopwords. \n",
    "# Sentence must be a list of words that make up the sentence\n",
    "# https://medium.com/@pemagrg/pre-processing-text-in-python-ad13ea544dae\n",
    "def process_sentence(sentence):\n",
    "    \n",
    "    # Process words in sentence\n",
    "    processed_sentence = [process_word(word) for word in sentence]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    processed_sentence = [word for word in processed_sentence if word not in stop_words]\n",
    "    \n",
    "    return processed_sentence\n",
    "\n",
    "\n",
    "# Similarity matrix\n",
    "def sentence_similarity(sent1, sent2):\n",
    "        \n",
    "    all_words = list(set(sent1 + sent2)) \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        #if w in stopwords:\n",
    "        #    continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        #if w in stopwords:\n",
    "        #    continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    \n",
    "    # Vectors must be non-zero to calculate cosine similarity\n",
    "    if vector1 == 0 or vector2 == 0:\n",
    "        return 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    processed_sentences = [process_sentence(sentence) for sentence in sentences]\n",
    "     \n",
    "    for idx1 in range(len(processed_sentences)):\n",
    "        for idx2 in range(len(processed_sentences)):\n",
    "            if idx1 == idx2: # ignore if both are same sentences\n",
    "                continue \n",
    "            sentences_similarity = sentence_similarity(processed_sentences[idx1], processed_sentences[idx2])\n",
    "            similarity_matrix[idx1][idx2] = sentences_similarity\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# Generate Summary Method\n",
    "def generate_summary_from_file(file_name, top_n=5, output=True):\n",
    "    # Read text and split it into sentences\n",
    "    file = open(file_name, \"r\")\n",
    "    file_data = file.readlines()\n",
    "    sentences = re.split(r'[?!.] ', file_data[0])\n",
    "    \n",
    "    long_sentences = create_long_sentences(sentences, output)\n",
    "    return generate_summary(long_sentences, top_n, output)\n",
    "\n",
    "\n",
    "def generate_summary_from_text(text, top_n=5, output=True):    \n",
    "    \n",
    "    if len(text) <= 200:\n",
    "        # If text is short, return the original text rather than a summary\n",
    "        return text\n",
    "    else:  \n",
    "        # Read text and split it into sentences\n",
    "        sentences = re.split(r'[?!.] ', text)\n",
    "        long_sentences = create_long_sentences(sentences, output)\n",
    "        return generate_summary(long_sentences, top_n, output)  \n",
    "    \n",
    "\n",
    "def generate_summary(sentences, top_n=5, output=True):\n",
    "    summarize_text = []\n",
    "    \n",
    "    # Generate Similarly Matrix across sentences\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences)\n",
    "\n",
    "    # Rank sentences in similarity matrix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    try: \n",
    "        # https://stackoverflow.com/questions/13040548/networkx-differences-between-pagerank-pagerank-numpy-and-pagerank-scipy\n",
    "        scores = nx.pagerank(sentence_similarity_graph) # The eigenvector calculation is done by the power iteration method and has no guarantee of convergence\n",
    "        #scores = nx.pagerank_numpy(sentence_similarity_graph) # The eigenvector calculation uses NumPyâ€™s interface to the LAPACK eigenvalue solvers. This will be the fastest and most accurate for small graphs.\n",
    "        #scores = nx.pagerank_scipy(sentence_similarity_graph) # SciPy sparse-matrix implementation of the power-method\n",
    "    except nx.NetworkXError:\n",
    "        print (\"NetworkXError\")\n",
    "        return \"NetworkXError\" # generate_text_rank_summary(text)\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        print (\"PowerIterationFailedConvergence\")\n",
    "        return \"PowerIterationFailedConvergence\" # generate_text_rank_summary(text)\n",
    "    \n",
    "    # Sort the rank and pick top sentences\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    if output:\n",
    "        print(\"Indexes of top ranked_sentence order are \", ranked_sentences)    \n",
    "\n",
    "    if len(ranked_sentences) < top_n:\n",
    "        top_n = len(ranked_sentences)\n",
    "        \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentences[i][1]))\n",
    "\n",
    "    # Output the summary text        \n",
    "    return '. '.join(summarize_text).replace(\"..\",\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence = ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', '\"With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']\n",
    "processed_sentence = process_sentence(sentence)\n",
    "print(processed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# let's begin\n",
    "generate_summary_from_file(os.getcwd() + \"\\\\test.txt\", 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generate_summary_from_text(grouped_text.iloc[4], 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generate_summary_from_text(grouped_text.iloc[2], 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generate_summary_from_text(report_text, 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over all text in data frame and add summary\n",
    "grouped_text_df['TextRankCustom'] = grouped_text_df.apply(lambda x: generate_summary_from_text(x.text, 3, False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_text_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Output of Document Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_text_df.to_excel('.\\\\DocumentSummary.xlsx', sheet_name='TextRank', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
