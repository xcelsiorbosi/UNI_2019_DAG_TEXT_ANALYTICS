{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus creation \n",
    "\n",
    "-Requirements\n",
    "    oTokenization \n",
    "        Translate sentences into meaningful blocks\n",
    "    oBuild Bag-of-words\n",
    "        Vectorized dictionary\n",
    "-Algorithms to explore\n",
    "    oSkip-gram\n",
    "    oWord-2-vec\n",
    "    oTf-idf\n",
    "    oHierachical Softmax\n",
    "    oNegative Sampling\n",
    "    oNaïve Softmax\n",
    "    oMorphological segmentation\n",
    "    oPart-of-speech tagging\n",
    "    oSentence breaking\n",
    "        -Topic detection \n",
    "    oRequirements\n",
    "        Stop Words\n",
    "        Regular expression tokenizer\n",
    "        Snowball stemmer\n",
    "    oAlgorithms to consider\n",
    "        Latent Dirichlet Allocation (LDA) topic\n",
    "        Skip Gram (context prediction) \n",
    "        Multinominal Naïve Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\".\\word_tree\\HansardFlatDatasetUpdated.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dfTemp = df[['qTextID', 'talkerTranscript']]\n",
    "dfTemp = dfTemp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dfTemp = df[['qtalkKind', 'textID', 'talkerRole', 'talkerName', 'talkerTranscript']]\n",
    "\n",
    "sentTokens = []\n",
    "\n",
    "for i in dfTemp['talkerTranscript']:\n",
    "    try:\n",
    "        a = sent_tokenize(i)\n",
    "        sentTokens.append(a)\n",
    "        print(a)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        sentTokens.append(\"blank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenize - Treebank tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wordTokens = []\n",
    "for i in sentTokens:\n",
    "    a = word_tokenize(i[0])\n",
    "    print(a)\n",
    "    wordTokens.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "listOfWordsWithoutStops = []\n",
    "for listOfWords in wordTokens:\n",
    "        #print(listOfWords)\n",
    "        listOfWordsWithoutStopsInstance = [word for word in listOfWords if word not in english_stops]\n",
    "        listOfWordsWithoutStops.append(listOfWordsWithoutStopsInstance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "listOfWordsWithoutStops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sysnets and hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('police')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "wordnet.synsets('police')[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech Tag\n",
    "Noun = n\n",
    "Adjective = a\n",
    "Adverb = r\n",
    "Verb = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas and synonyms\n",
    "#### A lemma (in linguistics), is the canonical form or morphological form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmas = syn.lemmas()\n",
    "loops = len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(loops):\n",
    "    print(lemmas[i].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmas[0].synset() == lemmas[1].synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('politician'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "len(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gn2 = wordnet.synset('good.n.02')\n",
    "gn2.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evil = gn2.lemmas()[0].antonyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evil.name\n",
    "evil.synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ga1 = wordnet.synset('good.a.01')\n",
    "ga1.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "bad = ga1.lemmas()[0].antonyms()[0]\n",
    "bad.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "bad.synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity - Wu-Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cb = wordnet.synset('police.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ib = wordnet.synset('constabulary.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cb.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ib.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cb.shortest_path_distance(ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover word collocations - (need to run pre stopwords) - also need to pick a larger text blob to get themes for the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "words = [w.lower() for w in webtext.words('grail.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pre stop words\n",
    "listOfWordsWithoutStopsBigramCollocationFinder = []\n",
    "for listOfWords in sentTokens:\n",
    "        #print(listOfWords)\n",
    "        listOfWordsWithoutStopsInstance = [word for word in listOfWords if word not in english_stops]\n",
    "        bcf = BigramCollocationFinder.from_words(listOfWordsWithoutStopsInstance)\n",
    "        listOfWordsWithoutStopsInstance =  bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n",
    "        \n",
    "        \n",
    "        listOfWordsWithoutStopsBigramCollocationFinder.append(listOfWordsWithoutStopsInstance)\n",
    "\n",
    "listOfWordsWithoutStopsBigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# post stop words\n",
    "listOfWordsWithoutStopsBigramCollocationFinder = []\n",
    "for listOfWords in listOfWordsWithoutStops:\n",
    "        #print(listOfWords)\n",
    "        listOfWordsWithoutStopsInstance = [word for word in listOfWords if word not in english_stops]\n",
    "        bcf = BigramCollocationFinder.from_words(listOfWordsWithoutStopsInstance)\n",
    "        listOfWordsWithoutStopsInstance =  bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n",
    "        listOfWordsWithoutStopsBigramCollocationFinder.append(listOfWordsWithoutStopsInstance)\n",
    "\n",
    "listOfWordsWithoutStopsBigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "allWords = []\n",
    "loops = len(listOfWordsWithoutStops)\n",
    "\n",
    "for i in range(loops):\n",
    "    words = [w.lower() for w in listOfWordsWithoutStops[i]]\n",
    "    allWords.append(words[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "allWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tcf = TrigramCollocationFinder.from_words(allWords)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "tcf.apply_freq_filter(3)\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words\n",
    "\n",
    "### Lemmatizing words with WordNet\n",
    "\n",
    "### Replacing words matching regular expressions\n",
    "\n",
    "### Removing repeating characters\n",
    "\n",
    "### Spelling correction with Enchant\n",
    "\n",
    "### Replacing synonyms\n",
    "\n",
    "### Replacing negations with antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('police')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmer.stem('politician')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancaster stemming algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmer.stem('runner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing')\n",
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmer.stem('runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmer.stem('Adelaide')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "spanish_stemmer = SnowballStemmer('english')\n",
    "spanish_stemmer.stem('howdy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing words with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('runners')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important example for difference between stemming and lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining stemming with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmer.stem('buses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('buses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmer.stem('bus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming saves one character, lemmatization saves two characters, and\n",
    "stemming the lemma saves a total of three characters out of five characters. That is nearly\n",
    "a 60% compression rate! This level of word compression over many thousands of words,\n",
    "while unlikely to always produce such high gains, can still make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "# class SpellingReplacer(object):\n",
    "#     def __init__(self, dict_name='en', max_dist=2):\n",
    "#         self.spell_dict = enchant.Dict(dict_name)\n",
    "#         self.max_dist = max_dist\n",
    "    \n",
    "#     def replace(self, word):\n",
    "#         if self.spell_dict.check(word):\n",
    "#             return word\n",
    "#         suggestions = self.spell_dict.suggest(word)\n",
    "    \n",
    "#         if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "#             return suggestions[0]\n",
    "#         else:\n",
    "#             return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from replacers import SpellingReplacer\n",
    "# replacer = SpellingReplacer()\n",
    "# replacer.replace('word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class WordReplacer(object):\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replacer = WordReplacer({'sapol': 'police'})\n",
    "replacer.replace('sapol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# requires a csv with 2 columns \n",
    "# word     |    synonym \n",
    "# police   |     sapol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = {}\n",
    "        for line in csv.reader(open(fname)):\n",
    "            word, syn = line\n",
    "            word_map[word] = syn\n",
    "        super(CsvWordReplacer, self).__init__(word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace words with  Antonym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replacer = AntonymReplacer()\n",
    "replacer.replace('nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replacer.replace('uglify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sent = [\"lets\", 'not', 'uglify', 'america', 'great', 'again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replacer.replace_negations(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class AntonymWordReplacer(WordReplacer, AntonymReplacer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replacer = AntonymWordReplacer({'evil': 'good'})\n",
    "replacer.replace_negations(['good', 'is', 'not', 'evil'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "path = os.path.expanduser('~/nltk_data')\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "os.path.exists(path)\n",
    "\n",
    "import nltk.data\n",
    "path in nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import WordListCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader = WordListCorpusReader(\".\",wordTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a = list(reader.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open('words.txt', 'w') as f:\n",
    "    for item in a:\n",
    "        print(item)\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import line_tokenize\n",
    "# line_tokenize(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('names')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "words.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import TaggedCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader = TaggedCorpusReader('.', r'.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# reader.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# reader.tagged_paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "reader = TaggedCorpusReader('.', r'.*\\.txt', word_tokenizer=SpaceTokenizer())\n",
    "reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import LineTokenizer\n",
    "reader = TaggedCorpusReader('.', r'.*\\.txt', sent_tokenizer=LineTokenizer())\n",
    "reader.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# universal tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reader = TaggedCorpusReader('.', r'.*\\.txt', tagset='en-brown')\n",
    "kk = reader.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
